{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d48e91d4",
   "metadata": {},
   "source": [
    "<h1>NLP - Language Modeling</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c309e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os.path\n",
    "from nltk.corpus import treebank\n",
    "from collections import Counter, defaultdict\n",
    "from random import choice, choices\n",
    "import numpy\n",
    "from math import log2, exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9530abdd",
   "metadata": {},
   "source": [
    "<h2>Language Modeling with N_Grams - Tokens</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169e2e2c",
   "metadata": {},
   "source": [
    "<h3>Prepairing training and testing data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e34dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching text files\n",
    "training_file_ids, testing_file_ids  = treebank.fileids()[:150], treebank.fileids()[150:]\n",
    "training_raw_text, testing_raw_text = treebank.sents(fileids=training_file_ids), treebank.sents(fileids=testing_file_ids)\n",
    "\n",
    "# Adding tokens to signal the begining and the end of sentences\n",
    "def add_sentence_boundar(list_of_sentences):\n",
    "    sb_list = []\n",
    "    for sentence_list in list_of_sentences:\n",
    "        sentence_list.insert(0, '<BOS>')\n",
    "        sentence_list.append('<EOS>')\n",
    "        sb_list.append(sentence_list)\n",
    "    return sb_list\n",
    "\n",
    "# Remove nltk wildcard chars\n",
    "def removing_wildcard_chars(list_of_sentences):\n",
    "    end_list = []\n",
    "    for sentence_list in list_of_sentences:\n",
    "        for token in sentence_list:\n",
    "            if '*' in token:\n",
    "                sentence_list.pop(sentence_list.index(token))\n",
    "        end_list.append(sentence_list)\n",
    "    return end_list\n",
    "\n",
    "# Unpack list of lists to a single list of tokens\n",
    "def unpack_lists(list_of_sentences):\n",
    "    return [item for sublist in list_of_sentences for item in sublist]\n",
    "\n",
    "# Replacing tokens that appear less than 3 times with the <UNK>\n",
    "def replace_low_freq_tokens(list_of_tokens, occ_times, char='<UNK>'):\n",
    "    if occ_times < 1:\n",
    "        occ_times = 1\n",
    "    for i in list_of_tokens:\n",
    "        if list_of_tokens.count(i) < occ_times:\n",
    "            list_of_tokens[list_of_tokens.index(i)] = char\n",
    "    return list_of_tokens\n",
    "\n",
    "# Extracting vocabulary\n",
    "def get_covab(list_of_tokens):\n",
    "    return list(set(list_of_tokens))\n",
    "\n",
    "def replace_if_not_in_vocab(list_of_tokens, vocab):\n",
    "    for i in list_of_tokens:\n",
    "        if i not in vocab:\n",
    "            list_of_tokens[list_of_tokens.index(i)] = '<UNK>'\n",
    "    return list_of_tokens\n",
    "\n",
    "# Extracting list out of text docs\n",
    "def txt_to_list(txt_path):\n",
    "    file_content = open(txt_path, 'r')\n",
    "    content_list = []\n",
    "    for line in file_content:\n",
    "        content_list.append(line.strip())\n",
    "    return(content_list)\n",
    "\n",
    "# Creating and storing processed data, if file doesn't already exist\n",
    "def storing_processed_text():\n",
    "    if not os.path.exists(\"training_tokens.txt\"):\n",
    "        training_list = replace_low_freq_tokens(unpack_lists(add_sentence_boundar(removing_wildcard_chars(training_raw_text))),3)\n",
    "        with open(\"training_tokens.txt\", \"w\") as outfile:\n",
    "            outfile.write(\"\\n\".join(training_list))\n",
    "\n",
    "    if not os.path.exists(\"testing_tokens.txt\"):\n",
    "        testing_list_with_unknown = unpack_lists(add_sentence_boundar(removing_wildcard_chars(testing_raw_text)))\n",
    "        try:\n",
    "            vocab = get_covab(training_list)\n",
    "        except:\n",
    "            training_list = txt_to_list(\"training_tokens.txt\")\n",
    "            vocab = get_covab(training_list)\n",
    "        testing_list = replace_if_not_in_vocab(testing_list_with_unknown, vocab)\n",
    "        with open(\"testing_tokens.txt\", \"w\") as outfile:\n",
    "            outfile.write(\"\\n\".join(testing_list))\n",
    "            \n",
    "storing_processed_text()\n",
    "training_list, test_list = txt_to_list(\"training_tokens.txt\"), txt_to_list(\"testing_tokens.txt\")\n",
    "vocab = get_covab(training_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd1372",
   "metadata": {},
   "source": [
    "<h3>Bigram Model - Tokens</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ec598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laplace Smoothing is set to default\n",
    "class Bigram():\n",
    "    def __init__(self):\n",
    "        self.ngram = 2\n",
    "        self.smoothing_type = 'Laplace'\n",
    "        self.bigrams = {}\n",
    "        self.bg_prob_dict = {}\n",
    "        self.n_unigrams = defaultdict(int)\n",
    "        self.n_bigrams = defaultdict(lambda: defaultdict(int))\n",
    "        self.len = []\n",
    "        self.perp = 0\n",
    "        \n",
    "    def fit(self, token_list):\n",
    "        for i in range(1, len(token_list)):\n",
    "            curr_bg = (token_list[i-1], token_list[i])\n",
    "            self.n_bigrams[curr_bg[0]][curr_bg[1]] += 1\n",
    "            self.n_unigrams[curr_bg[0]] += 1\n",
    "\n",
    "            \n",
    "    def update_len(self, token_list):\n",
    "        for i in range(1, len(token_list)):\n",
    "            bigram = (token_list[i-1], token_list[i])\n",
    "            self.len.append(self.n_bigrams[bigram[0]][bigram[1]])\n",
    "        n_bg = {}\n",
    "        for token in self.len:\n",
    "            if token in n_bg:\n",
    "               n_bg[token] += 1\n",
    "            else:\n",
    "               n_bg[token] = 1\n",
    "        return n_bg\n",
    "    \n",
    "    def get_bigram_prob(self, token_list):\n",
    "            pstar, counter = 0, 0\n",
    "            for i in range(1, len(token_list)):\n",
    "                bigram = (token_list[i-1], token_list[i])\n",
    "                w1, w2 = bigram[0], bigram[1]\n",
    "                if self.smoothing_type == 'Laplace' or self.smoothing_type == 'Add 1':\n",
    "                    pstar = 1.0 * self.n_bigrams[w1][w2] / self.n_unigrams[w1]\n",
    "                    self.bg_prob_dict[bigram] = pstar\n",
    "                elif self.smoothing_type == 'Good-Turing Discounting':\n",
    "                    c = self.len[self.n_bigrams[w1][w1]] \n",
    "                    n_next = c + 1 \n",
    "                    cstar = (self.n_bigrams[w2][w2] + 1) * n_next  / c\n",
    "                    pstar = cstar/len(self.len)\n",
    "                    self.bg_prob_dict[bigram] = pstar\n",
    "            if self.smoothing_type == 'Good-Turing Discounting':\n",
    "                self.update_len(token_list)\n",
    "            return self.bg_prob_dict\n",
    "        \n",
    "    def predict_next_word(self, last_word):\n",
    "        next_word = {}\n",
    "        for k in self.bg_prob_dict:\n",
    "            if k[0] == last_word:\n",
    "                next_word[k[1]] = self.bg_prob_dict[k]\n",
    "        k = Counter(next_word)\n",
    "        high = k.most_common()\n",
    "        answer = ''\n",
    "        while answer == '' or answer == '<UNK>':\n",
    "            try:\n",
    "                if high[0][0] == '<UNK>' and len(high) == 1:\n",
    "                    answer = choice(vocab)\n",
    "                else:\n",
    "                    answer = choices([i[0] for i in high], weights=[i[1] for i in high], k=1)[0]\n",
    "            except:\n",
    "                answer = choice(vocab)\n",
    "        return answer\n",
    "    \n",
    "    def create_sentence_start(self):\n",
    "        start_list = ['<BOS>']\n",
    "        start_list.append(self.predict_next_word('<BOS>'))\n",
    "        return start_list\n",
    "    \n",
    "    def generate_sentence(self):\n",
    "        curr_sentence = self.create_sentence_start()\n",
    "        last_word = curr_sentence[-1]\n",
    "        while last_word != '<EOS>':\n",
    "            new_last_word = self.predict_next_word(last_word)\n",
    "            curr_sentence.append(new_last_word)\n",
    "            last_word = new_last_word\n",
    "        return curr_sentence\n",
    "    \n",
    "    def perplexity(self, token_list):\n",
    "        perp = 1\n",
    "        prob = 1\n",
    "        V = len(token_list)\n",
    "        for i in range(1,len(token_list)):\n",
    "            bg = (token_list[i-1],token_list[i])\n",
    "            if bg in self.bg_prob_dict.keys():\n",
    "               prob += log2(self.bg_prob_dict[bg])\n",
    "        perp = (-prob/V)*100\n",
    "        self.perp = perp\n",
    "        return perp\n",
    "    \n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd96250",
   "metadata": {},
   "source": [
    "<h3>Trigram Model - Tokens</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e0d866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laplace Smoothing is set to default\n",
    "class Trigram():\n",
    "    def __init__(self):\n",
    "        self.ngram = 3\n",
    "        self.smoothing_type = 'Laplace'\n",
    "        self.bigrams = {}\n",
    "        self.trigrams = {}\n",
    "        self.tg_prob_dict = {}\n",
    "        self.n_unigrams = defaultdict(int)\n",
    "        self.n_bigrams = defaultdict(lambda: defaultdict(int))\n",
    "        self.n_trigrams = defaultdict(lambda: defaultdict(int))\n",
    "        self.len = []\n",
    "        self.perp = 0\n",
    "        \n",
    "    def fit(self, token_list):\n",
    "        for i in range(1, len(token_list)):\n",
    "            curr_tg = (token_list[i-2], token_list[i-1],  token_list[i])\n",
    "            self.n_trigrams[(curr_tg[0],curr_tg[1])][curr_tg[2]] += 1\n",
    "            self.n_bigrams[curr_tg[0]][curr_tg[1]] += 1\n",
    "            self.n_unigrams[curr_tg[0]] += 1\n",
    "\n",
    "    def update_len(self, token_list):\n",
    "        for i in range(1, len(token_list)):\n",
    "            trigram = (token_list[i-2],token_list[i-1], token_list[i])\n",
    "            self.len.append(self.n_trigrams[(trigram[0],trigram[1])][trigram[2]])\n",
    "        n_tg = {}\n",
    "        for token in self.len:\n",
    "            if token in n_tg:\n",
    "               n_tg[token] += 1\n",
    "            else:\n",
    "               n_tg[token] = 1\n",
    "        return n_tg\n",
    "    \n",
    "    def get_trigram_prob(self, token_list):\n",
    "        pstar, counter = 0, 0\n",
    "        for i in range(1, len(token_list)):\n",
    "            trigram = (token_list[i-2],token_list[i-1], token_list[i])\n",
    "\n",
    "            w1, w2, w3 = trigram[0], trigram[1], trigram[2]\n",
    "            if self.smoothing_type == 'Laplace' or self.smoothing_type == 'Add 1':\n",
    "                pstar = 1.0 * self.n_trigrams[(w1,w2)][w3] / self.n_bigrams[w1][w2]\n",
    "                self.tg_prob_dict[trigram] = pstar\n",
    "            elif self.smoothing_type == 'Good-Turing Discounting':\n",
    "                c = self.len[self.n_trigrams[(trigram[0],trigram[1])][trigram[2]]] \n",
    "                n_next = c + 1 \n",
    "                cstar = (self.n_trigrams[(trigram[0],trigram[1])][trigram[2]] + 1) * n_next  / c\n",
    "                pstar = cstar/len(self.len)\n",
    "                self.tg_prob_dict[trigram] = pstar\n",
    "        if self.smoothing_type == 'Good-Turing Discounting':\n",
    "            self.update_len(token_list)\n",
    "        return self.tg_prob_dict\n",
    "\n",
    "    def predict_next_word(self, last_word):\n",
    "        next_word = {}\n",
    "        for k in self.tg_prob_dict:\n",
    "            if [k[0], k[1]] == last_word:\n",
    "                next_word[k[2]] = self.tg_prob_dict[k]\n",
    "        k = Counter(next_word)\n",
    "        high = k.most_common()\n",
    "        answer = ''\n",
    "        while answer == '':\n",
    "            if len(high) == 0 or high[0][0] == '<UNK>':\n",
    "                answer = choice(vocab)\n",
    "                return answer\n",
    "            else:\n",
    "                answer = choices([i[0] for i in high], weights=[i[1] for i in high], k=1)[0]\n",
    "                return answer\n",
    "\n",
    "    \n",
    "    def create_sentence_start(self):\n",
    "        start_list = ['<BOS>']\n",
    "        bg = Bigram()\n",
    "        bg.smoothing_type = 'Laplace'\n",
    "        bg.fit(training_list)\n",
    "        bg.bigrams = bg.get_bigram_prob(training_list)\n",
    "        start_list.append(bg.predict_next_word('<BOS>'))\n",
    "        return start_list\n",
    "    \n",
    "    def generate_sentence(self):\n",
    "        curr_sentence = self.create_sentence_start()\n",
    "        last_word = curr_sentence\n",
    "        while last_word != '<EOS>':\n",
    "            new_last_word = self.predict_next_word(last_word)\n",
    "            curr_sentence.append(new_last_word)\n",
    "        return curr_sentence\n",
    "        \n",
    "    def perplexity(self, token_list):\n",
    "        perp = 1\n",
    "        prob = 1\n",
    "        V = len(token_list)\n",
    "        for i in range(2, len(token_list)):\n",
    "            tg = (token_list[i-2],token_list[i-1], token_list[i])\n",
    "            if tg in self.tg_prob_dict.keys():\n",
    "               prob += log2(self.tg_prob_dict[tg])\n",
    "        perp = (prob*100/V)*-1\n",
    "        self.perp = perp\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e952a2",
   "metadata": {},
   "source": [
    "<h3>Creating and Testing the models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99a1fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Bigram with Laplace Smoothing perplexity: 326.63\n",
      "Bigram with Good-Turing Discounting Smoothing perplexity: 1098.92\n",
      "Trigram with Laplace Smoothing perplexity: 122.19\n",
      "Trigram with Good-Turing Discounting Smoothing perplexity: 531.95\n",
      "\n",
      "The Trigram model with Laplace Smoothing has the least perplexity and thus is the better performance\n",
      "\n",
      "[['<BOS>', 'New', 'York', 'Stock', 'Exchange', 'chairman', ',', 'it', 'with', 'a', 'bad', 'day', \"'s\", 'office', 'during', 'World', 'Bank', 'of', 'all', ',', 'but', 'would', 'want', 'the', 'provision', ',', 'where', 'the', 'firm', 'and', 'gave', 'up', 'that', '``', 'Markey', ',', \"''\", '<EOS>'], ['<BOS>', '``', 'designed', '*-2', 'slow', 'program', 'trading', '.', '<EOS>'], ['<BOS>', 'They', 'say', '0', 'it', 'a', 'deal', ',', 'the', 'sixth', 'consecutive', 'months', 'of', '$', '2.2', 'billion', '.', '<EOS>']]\n"
     ]
    }
   ],
   "source": [
    "# Creating a bigram with laplace smoothing and one with good-turing discounting\n",
    "laplace_bigram, good_turing_bigram = Bigram(), Bigram()\n",
    "laplace_bigram.smoothing_type, good_turing_bigram.smoothing_type = 'Laplace', 'Good-Turing Discounting'\n",
    "laplace_bigram.fit(training_list)\n",
    "laplace_bigram.bigrams = laplace_bigram.get_bigram_prob(training_list)\n",
    "laplace_bigram.perplexity(test_list)\n",
    " \n",
    "good_turing_bigram.fit(training_list)\n",
    "good_turing_bigram.update_len(training_list)\n",
    "good_turing_bigram.bigrams = good_turing_bigram.get_bigram_prob(training_list)\n",
    "good_turing_bigram.perplexity(test_list)   \n",
    "\n",
    "# Creating a trigram with laplace smoothing and one with good-turing discounting\n",
    "laplace_trigram, good_turing_trigram = Trigram(), Trigram()\n",
    "laplace_trigram.smoothing_type, good_turing_trigram.smoothing_type = 'Laplace', 'Good-Turing Discounting'\n",
    "laplace_trigram.fit(training_list)\n",
    "laplace_trigram.trigrams = laplace_trigram.get_trigram_prob(training_list)\n",
    "laplace_trigram.perplexity(test_list)\n",
    "good_turing_trigram.fit(training_list)\n",
    "good_turing_trigram.update_len(training_list)\n",
    "good_turing_trigram.trigrams = good_turing_trigram.get_trigram_prob(training_list)\n",
    "good_turing_trigram.perplexity(test_list)\n",
    "print('Results:')\n",
    "print('Bigram with Laplace Smoothing perplexity: ' + str(round(laplace_bigram.perp,2)))\n",
    "print('Bigram with Good-Turing Discounting Smoothing perplexity: ' + str(round(good_turing_bigram.perp,2)))\n",
    "print('Trigram with Laplace Smoothing perplexity: ' + str(round(laplace_trigram.perp,2)))\n",
    "print('Trigram with Good-Turing Discounting Smoothing perplexity: ' + str(round(good_turing_trigram.perp,2)))\n",
    "print('\\nThe Trigram model with Laplace Smoothing has the least perplexity and thus is the better performance\\n')\n",
    "\n",
    "print([laplace_bigram.generate_sentence(),laplace_bigram.generate_sentence(), laplace_bigram.generate_sentence()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d924239",
   "metadata": {},
   "source": [
    "<h2>Language Modeling with N_Grams - Characters</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e39a5c",
   "metadata": {},
   "source": [
    "<h3>Prepairing training and testing data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e1d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vima_training_names = ['VIMA001.TXT', 'VIMA002.TXT', 'VIMA003.TXT', 'VIMA004.TXT', 'VIMA005.TXT', 'VIMA006.TXT', 'VIMA007.TXT','VIMA008.TXT','VIMA009.TXT','VIMA0010.TXT']\n",
    "vima_testing_names = ['VIMA011.TXT', 'VIMA012.TXT']\n",
    "\n",
    "def extract_vima_text(name_list):\n",
    "    vima_texts = [(open('../Assignment-1/assignment1textfiles/sbd/' + i, encoding=\"utf8\").read().split('\\n')) for i in os.listdir('../Assignment-1/assignment1textfiles/sbd') if i in name_list]\n",
    "\n",
    "    end_list = []\n",
    "    for text in vima_texts:\n",
    "        for token in text:\n",
    "            # Adding symbols for start and end of token\n",
    "            new_token = list(token)\n",
    "            new_token.insert(0,'@')\n",
    "            new_token.append('~')\n",
    "            end_list.append(new_token)\n",
    "            \n",
    "    # Unpack lists\n",
    "    end_list=unpack_lists(end_list)\n",
    "    # Makes no difference\n",
    "    # print(len(end_list))\n",
    "    # replace_low_freq_tokens(end_list, 5, '*')\n",
    "    # print(len(end_list))\n",
    "    return end_list\n",
    "\n",
    "vima_training_list = extract_vima_text(vima_training_names)\n",
    "vima_testing_list = extract_vima_text(vima_testing_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64382b74",
   "metadata": {},
   "source": [
    "<h3>Trigram Model - Characters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "213264d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Trigram with Laplace Smoothing perplexity: 226.32\n",
      "Character Trigram with Good-Turing Discounting Smoothing perplexity: 923.89\n"
     ]
    }
   ],
   "source": [
    "# Triagram class given as declared above is sufficient\n",
    "char_trigram_laplace = Trigram()\n",
    "char_trigram_laplace.smoothing_type = 'Laplace'\n",
    "char_trigram_laplace.fit(vima_training_list)\n",
    "char_trigram_laplace.trigrams = char_trigram_laplace.get_trigram_prob(vima_training_list)\n",
    "char_trigram_laplace.perplexity(vima_testing_list)\n",
    "\n",
    "char_trigram_gt = Trigram()\n",
    "char_trigram_gt.smoothing_type = 'Good-Turing Discounting'\n",
    "char_trigram_gt.fit(vima_training_list)\n",
    "char_trigram_gt.update_len(vima_training_list)\n",
    "char_trigram_gt.trigrams = char_trigram_gt.get_trigram_prob(vima_training_list)\n",
    "char_trigram_gt.perplexity(vima_testing_list)\n",
    "\n",
    "print('Character Trigram with Laplace Smoothing perplexity: ' + str(round(char_trigram_laplace.perp,2)))\n",
    "print('Character Trigram with Good-Turing Discounting Smoothing perplexity: ' + str(round(char_trigram_gt.perp,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56612aa",
   "metadata": {},
   "source": [
    "<h3>Four-Gram Model - Characters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "500051aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fourgram():\n",
    "    def __init__(self):\n",
    "        self.ngram = 4\n",
    "        self.smoothing_type = 'Laplace'\n",
    "        self.bigrams = {}\n",
    "        self.trigrams = {}\n",
    "        self.fourgrams = {}\n",
    "        self.fg_prob_dict = {}\n",
    "        self.n_unigrams = defaultdict(int)\n",
    "        self.n_bigrams = defaultdict(lambda: defaultdict(int))\n",
    "        self.n_trigrams = defaultdict(lambda: defaultdict(int))\n",
    "        self.n_fourgrams = defaultdict(lambda: defaultdict(int))\n",
    "        self.len = []\n",
    "        self.perp = 0\n",
    "        \n",
    "    def fit(self, token_list):\n",
    "        for i in range(1, len(token_list)):\n",
    "            curr_fg = (token_list[i-3], token_list[i-2],  token_list[i-1],  token_list[i])\n",
    "            self.n_fourgrams[(curr_fg[0],curr_fg[1],curr_fg[2])][curr_fg[3]] += 1\n",
    "            self.n_trigrams[(curr_fg[0],curr_fg[1])][curr_fg[2]] += 1\n",
    "            self.n_bigrams[curr_fg[0]][curr_fg[1]] += 1\n",
    "            self.n_unigrams[curr_fg[0]] += 1\n",
    "\n",
    "    def update_len(self, token_list):\n",
    "        for i in range(1, len(token_list)):\n",
    "            fourgram = (token_list[i-3], token_list[i-2],token_list[i-1], token_list[i])\n",
    "            self.len.append(self.n_fourgrams[(fourgram[0],fourgram[1], fourgram[2])][fourgram[3]])\n",
    "        n_fg = {}\n",
    "        for token in self.len:\n",
    "            if token in n_fg:\n",
    "               n_fg[token] += 1\n",
    "            else:\n",
    "               n_fg[token] = 1\n",
    "        return n_fg\n",
    "    \n",
    "    def get_fourgram_prob(self, token_list):\n",
    "        pstar, counter = 0, 0\n",
    "        for i in range(1, len(token_list)):\n",
    "            fourgram = (token_list[i-3], token_list[i-2],token_list[i-1], token_list[i])\n",
    "            w1, w2, w3, w4 = fourgram[0], fourgram[1], fourgram[2], fourgram[3]\n",
    "            if self.smoothing_type == 'Laplace' or self.smoothing_type == 'Add 1':\n",
    "                pstar = 1.0 * self.n_fourgrams[(w1,w2,w3)][w4] / self.n_trigrams[(w1,w2)][w3]\n",
    "                self.fg_prob_dict[fourgram] = pstar\n",
    "            elif self.smoothing_type == 'Good-Turing Discounting':\n",
    "                c = self.len[self.n_fourgrams[(w1,w2,w3)][w4]] \n",
    "                n_next = c + 1 \n",
    "                cstar = (self.n_fourgrams[(w1,w2,w3)][w4] + 1) * n_next  / c\n",
    "                pstar = cstar/len(self.len)\n",
    "                self.fg_prob_dict[fourgram] = pstar\n",
    "        if self.smoothing_type == 'Good-Turing Discounting':\n",
    "            self.update_len(token_list)\n",
    "        return self.fg_prob_dict\n",
    "        \n",
    "    def perplexity(self, token_list):\n",
    "        perp = 1\n",
    "        prob = 1\n",
    "        V = len(token_list)\n",
    "        for i in range(3, len(token_list)):\n",
    "            fg = (token_list[i-3], token_list[i-2],token_list[i-1], token_list[i])\n",
    "            if fg in self.fg_prob_dict.keys():\n",
    "               prob += log2(self.fg_prob_dict[fg])\n",
    "        perp = (prob*100/V)*-1\n",
    "        self.perp = perp\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0119288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Trigram with Laplace Smoothing perplexity: 226.32\n",
      "Character Trigram with Good-Turing Discounting Smoothing perplexity: 923.89\n",
      "4-gram with Laplace Smoothing perplexity: 148.32\n",
      "4-gram Trigram with Good-Turing Discounting Smoothing perplexity: 964.05\n",
      "\n",
      "4-grams with Laplace smoothing seem to have the best performance.\n"
     ]
    }
   ],
   "source": [
    "char_trigram_laplace = Trigram()\n",
    "char_trigram_laplace.smoothing_type = 'Laplace'\n",
    "char_trigram_laplace.fit(vima_training_list)\n",
    "char_trigram_laplace.trigrams = char_trigram_laplace.get_trigram_prob(vima_training_list)\n",
    "char_trigram_laplace.perplexity(vima_testing_list)\n",
    "\n",
    "char_trigram_gt = Trigram()\n",
    "char_trigram_gt.smoothing_type = 'Good-Turing Discounting'\n",
    "char_trigram_gt.fit(vima_training_list)\n",
    "char_trigram_gt.update_len(vima_training_list)\n",
    "char_trigram_gt.trigrams = char_trigram_gt.get_trigram_prob(vima_training_list)\n",
    "char_trigram_gt.perplexity(vima_testing_list)\n",
    "\n",
    "\n",
    "char_fgram_laplace = Fourgram()\n",
    "char_fgram_laplace.smoothing_type = 'Laplace'\n",
    "char_fgram_laplace.fit(vima_training_list)\n",
    "char_fgram_laplace.trigrams = char_fgram_laplace.get_fourgram_prob(vima_training_list)\n",
    "char_fgram_laplace.perplexity(vima_testing_list)\n",
    "\n",
    "\n",
    "char_fgram_gt = Fourgram()\n",
    "char_fgram_gt.smoothing_type = 'Good-Turing Discounting'\n",
    "char_fgram_gt.fit(vima_training_list)\n",
    "char_fgram_gt.update_len(vima_training_list)\n",
    "char_fgram_gt.fourgrams = char_fgram_gt.get_fourgram_prob(vima_training_list)\n",
    "char_fgram_gt.perplexity(vima_testing_list)\n",
    "\n",
    "print('Character Trigram with Laplace Smoothing perplexity: ' + str(round(char_trigram_laplace.perp,2)))\n",
    "print('Character Trigram with Good-Turing Discounting Smoothing perplexity: ' + str(round(char_trigram_gt.perp,2)))\n",
    "\n",
    "print('4-gram with Laplace Smoothing perplexity: ' + str(round(char_fgram_laplace.perp,2)))\n",
    "print('4-gram Trigram with Good-Turing Discounting Smoothing perplexity: ' + str(round(char_fgram_gt.perp,2)))\n",
    "\n",
    "print('\\n4-grams with Laplace smoothing seem to have the best performance.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ef0294",
   "metadata": {},
   "source": [
    "<h2>Predicting Next Charachter with Neural Networks</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6ec5c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278aafad",
   "metadata": {},
   "source": [
    "<h3>Utils and Data Preparation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b700b0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27  1  5 83  5 90 63 90 69 93]\n"
     ]
    }
   ],
   "source": [
    "# Utility functions for our NNs\n",
    "\n",
    "# chars to ints - outputs np.ndarray object\n",
    "def encode_strs(str):\n",
    "    chars = tuple(set(str))\n",
    "    int2char = dict(enumerate((chars)))\n",
    "    char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "    encoded = np.array([char2int[ch] for ch in str])     \n",
    "    return encoded\n",
    "\n",
    "# one hot encoder to encode our array of nums\n",
    "def one_hot_encode(arr, n_labels):\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype = np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    return one_hot\n",
    "\n",
    "# Util function to create batches\n",
    "def get_batches(arr, batch_size, seq_length):\n",
    "    batch_size_total = batch_size*seq_length\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    arr = arr[:n_batches*batch_size_total]\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:,:-1], y[:,-1] = x[:,1:], arr[:, n+seq_length]\n",
    "        # Accounting for indexerror\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y \n",
    "        \n",
    "# Prepairing the data\n",
    "training_data = ''.join(extract_vima_text(vima_training_names[:8]))\n",
    "validation_data = ''.join(extract_vima_text(vima_training_names[8:]))\n",
    "test_data = ''.join(extract_vima_text(vima_testing_names))\n",
    "\n",
    "training_data, validation_data, test_data = encode_strs(training_data), encode_strs(validation_data), encode_strs(test_data)\n",
    "print(training_data[:10])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37ded93",
   "metadata": {},
   "source": [
    "<h3>LSTM for character prediction</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f32234f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FF_NET(nn.Module):\n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=10, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.linear = nn.Linear(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        r_output, hidden = self.linear(x, hidden)\n",
    "\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(), weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "class LSTM_RNN(nn.Module):\n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=10, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(), weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ea1db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a net\n",
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "#     val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[0], data[1]\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            \n",
    "            counter += 1\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            # print(counter%print_every)\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    \n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    inputs, targets = x, y\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train()\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                    # \"Val Loss: {:.4f}\".format(np.mean(val_losses)),\n",
    "                      \"Perplexity: {:.4f}...\".format(exp(loss.item())))\n",
    "\n",
    "# Predict given a net\n",
    "def predict(net, char):\n",
    "    x = np.array([net.char2int[str(char)[0]]])\n",
    "    x = one_hot_encode(x, len(net.chars))\n",
    "    inputs = torch.from_numpy(x)\n",
    "    \n",
    "    h = tuple([each.data for each in h])\n",
    "    out, h = net(inputs, h)\n",
    "    \n",
    "    p = F.softmax(out, dim =1).data\n",
    "\n",
    "    top_choice = np.arange(len(net.chars))\n",
    "        \n",
    "    p = p.numpy().squeeze()\n",
    "    char = np.random.choice(top_choice, p=p/p.sum())\n",
    "    \n",
    "    return net.int2char[char], h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ffae415",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers=10\n",
    "# setting the size of layers to 512 to experiment with performance\n",
    "n_hidden=512\n",
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 10\n",
    "net = LSTM_RNN(training_data, n_hidden, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3971ec77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 2... Loss: 10.9370... Perplexity: 56219.7368...\n",
      "Epoch: 1/10... Step: 4... Loss: 9.1261... Perplexity: 9191.9850...\n",
      "Epoch: 2/10... Step: 6... Loss: 6.9943... Perplexity: 1090.3958...\n",
      "Epoch: 2/10... Step: 8... Loss: 5.1973... Perplexity: 180.7847...\n",
      "Epoch: 3/10... Step: 10... Loss: 4.2179... Perplexity: 67.8913...\n",
      "Epoch: 3/10... Step: 12... Loss: 3.6562... Perplexity: 38.7148...\n",
      "Epoch: 4/10... Step: 14... Loss: 3.4761... Perplexity: 32.3335...\n",
      "Epoch: 4/10... Step: 16... Loss: 3.4244... Perplexity: 30.7057...\n",
      "Epoch: 5/10... Step: 18... Loss: 3.5254... Perplexity: 33.9688...\n",
      "Epoch: 5/10... Step: 20... Loss: 3.4893... Perplexity: 32.7615...\n",
      "Epoch: 6/10... Step: 22... Loss: 3.4824... Perplexity: 32.5376...\n",
      "Epoch: 6/10... Step: 24... Loss: 3.3653... Perplexity: 28.9435...\n",
      "Epoch: 7/10... Step: 26... Loss: 3.3738... Perplexity: 29.1903...\n",
      "Epoch: 7/10... Step: 28... Loss: 3.3495... Perplexity: 28.4879...\n",
      "Epoch: 8/10... Step: 30... Loss: 3.4105... Perplexity: 30.2798...\n",
      "Epoch: 8/10... Step: 32... Loss: 3.3550... Perplexity: 28.6449...\n",
      "Epoch: 9/10... Step: 34... Loss: 3.3744... Perplexity: 29.2056...\n",
      "Epoch: 9/10... Step: 36... Loss: 3.3248... Perplexity: 27.7947...\n",
      "Epoch: 10/10... Step: 38... Loss: 3.3750... Perplexity: 29.2241...\n",
      "Epoch: 10/10... Step: 40... Loss: 3.3209... Perplexity: 27.6850...\n"
     ]
    }
   ],
   "source": [
    "train(net, [training_data,validation_data], epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56459b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# storing and retrieving model\n",
    "checkpoint = {\n",
    "    'n_hidden': net.n_hidden,\n",
    "    'n_layers': net.n_layers,\n",
    "    'state_dict': net.state_dict(),\n",
    "    'tokens' : net.chars\n",
    "}\n",
    "\n",
    "with open('char_predict_lstm.net', 'wb') as f:\n",
    "    torch.save(checkpoint, f)\n",
    "\n",
    "with open('char_predict_lstm.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = LSTM_RNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
