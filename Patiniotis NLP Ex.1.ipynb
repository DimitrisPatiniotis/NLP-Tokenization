{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e552989",
   "metadata": {},
   "source": [
    "<h1>NLP - Tokenization and Sentence Boundary Disambiguation</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf849f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for tokenization task\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import treebank\n",
    "import spacy\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad1da6c",
   "metadata": {},
   "source": [
    "<h2>Tokenization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bc3127",
   "metadata": {},
   "source": [
    "<h3>Function to extract tokenization metrics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f5e2521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get intercection between correct tokenization and others, the missing tokens as well as the wrong tokens contained in other tokenizations\n",
    "def get_tokenization_accuracy(correct_tokenization, other_tokenization):\n",
    "    unified_set = set(list(itertools.chain(correct_tokenization, other_tokenization)))\n",
    "    ot_set = set(other_tokenization)\n",
    "    cor_set = set(correct_tokenization)\n",
    "    intersection_len = ot_set.intersection(cor_set)\n",
    "    int_perc = round((len(intersection_len)*100 / len(unified_set)),2)\n",
    "    missing_tokens = list(cor_set.difference(ot_set))\n",
    "    wrong_tokens = list(ot_set.difference(cor_set))\n",
    "    return int_perc, missing_tokens, wrong_tokens\n",
    "\n",
    "# Get List of total number of types\n",
    "def get_tokenization_lens(tok_list):\n",
    "    return [len(i) for i in tok_list]\n",
    "\n",
    "# Get list of total number of types\n",
    "def get_type_lens(tok_list):\n",
    "    return [len(set(i)) for i in tok_list]\n",
    "\n",
    "# Get 30 most common tokens for each tokenizaiton\n",
    "def get_ind_common_tokens(tok_list):\n",
    "    return list(dict.fromkeys([item for items, c in Counter(tok_list).most_common() for item in [items] * c]))[:30]\n",
    "\n",
    "# Get 30 most common tokens amongst all tokenizaitons\n",
    "def get_total_common_tokens(tok_list):\n",
    "    all_tokens = []\n",
    "    for i in tok_list:\n",
    "        all_tokens= all_tokens + i\n",
    "    return list(dict.fromkeys([item for items, c in Counter(all_tokens).most_common() for item in [items] * c]))[:30]\n",
    "\n",
    "# Get percent of tokens appearing only once\n",
    "def get_unique_token_percentage(ind_list):\n",
    "    unique_instances = len([i for i in ind_list if ind_list.count(i)==1])\n",
    "    unique_instances_perc = round((unique_instances*100/len(set(ind_list))),2)\n",
    "    return unique_instances_perc\n",
    "\n",
    "def batch_unique_token_percentages(list_of_lists):\n",
    "    return [get_unique_token_percentage(i) for i in list_of_lists]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00a6953",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786212fb",
   "metadata": {},
   "source": [
    "<h3>Getting Correct Tokenizations</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc36f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get correct Tokenizations\n",
    "wjs_correct_tokenization = [item for sublist in [i for i in treebank.sents(treebank.fileids())] for item in sublist]\n",
    "vima_correct_tokenization = [i for i in (''.join([(open('assignment1textfiles/sbd/' + i, encoding=\"utf8\").read()) for i in os.listdir('assignment1textfiles/sbd')]).replace('<S>', '').split('\\n')) if i != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f0b43a",
   "metadata": {},
   "source": [
    "<h3>Tokenization when dealing with English</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f9cddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Wall Street Journal\n",
    "wsj_raw_text = open(\"assignment1textfiles/wsj_untokenized.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "204a7cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk word_tokenize\n",
    "wsj_nltk_word_tokenize = word_tokenize(wsj_raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5379bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk wordpunct_tokenize\n",
    "wsj_nltk_tokenize_wordpunct_tokenize = wordpunct_tokenize(wsj_raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca913db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',']\n"
     ]
    }
   ],
   "source": [
    "# spacy tokenization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_en_tokens = [i.text for i in nlp(wsj_raw_text.strip())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4c5145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom regex tokenization\n",
    "regex_en_tokens = re.findall(\"[-'()]|[^a-z0-9 ](?= )|(?:[a-z0-9]|[^-'()a-z0-9 ](?! ))+\", wsj_raw_text, re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b2cf88",
   "metadata": {},
   "source": [
    "<h3>Tokenization when dealing with Greek</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f943016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all texts from Vima\n",
    "import os\n",
    "vima = ''.join([(open('assignment1textfiles/raw/' + i, encoding=\"utf8\").read()) for i in os.listdir('assignment1textfiles/raw')]).replace('\\n', ' ')                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e0a1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk word_tokenize\n",
    "vima_nltk_word_tokenize = word_tokenize(vima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7361109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk wordpunct_tokenize\n",
    "vima_nltk_tokenize_wordpunct_tokenize = wordpunct_tokenize(vima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee09dc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy tokenization\n",
    "nlpg = spacy.load(\"el_core_news_sm\")\n",
    "spacy_gr_tokens = [i.text for i in nlpg(vima)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2cecfce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# custom regex tokenization\n",
    "regex_gr_tokens = re.findall(\"[-'()]|[^α-ω0-9 ](?= )|(?:[α-ωόάώήί0-9]|[^-'()α-ω0-9 ](?! ))+\", vima, re.IGNORECASE)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d73c9b",
   "metadata": {},
   "source": [
    "<h3>Tokenization Comparison</h3>\n",
    "<h4>Wall Street Journal Tokenization<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5fe44357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall Street Journal Tokenization Results:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenization Accuracy</th>\n",
       "      <th>Token List Length</th>\n",
       "      <th>Type List Length</th>\n",
       "      <th>Unique Token Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ground Truth</th>\n",
       "      <td>100.0%</td>\n",
       "      <td>100676</td>\n",
       "      <td>12408</td>\n",
       "      <td>52.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nltk word_tokenize</th>\n",
       "      <td>93.78%</td>\n",
       "      <td>93530</td>\n",
       "      <td>12000</td>\n",
       "      <td>52.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nltk tokenize.wordpunct_tokenize</th>\n",
       "      <td>80.09%</td>\n",
       "      <td>100637</td>\n",
       "      <td>10962</td>\n",
       "      <td>48.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spacy Tokenization</th>\n",
       "      <td>86.5%</td>\n",
       "      <td>95892</td>\n",
       "      <td>11477</td>\n",
       "      <td>50.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Custom Regex Tokenization</th>\n",
       "      <td>77.25%</td>\n",
       "      <td>98753</td>\n",
       "      <td>12021</td>\n",
       "      <td>52.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Tokenization Accuracy  Token List Length  \\\n",
       "Ground Truth                                    100.0%             100676   \n",
       "Nltk word_tokenize                              93.78%              93530   \n",
       "Nltk tokenize.wordpunct_tokenize                80.09%             100637   \n",
       "Spacy Tokenization                               86.5%              95892   \n",
       "Custom Regex Tokenization                       77.25%              98753   \n",
       "\n",
       "                                  Type List Length  Unique Token Percentage  \n",
       "Ground Truth                                 12408                    52.09  \n",
       "Nltk word_tokenize                           12000                    52.12  \n",
       "Nltk tokenize.wordpunct_tokenize             10962                    48.31  \n",
       "Spacy Tokenization                           11477                    50.07  \n",
       "Custom Regex Tokenization                    12021                    52.07  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsj_tokenizations = [wjs_correct_tokenization, wsj_nltk_word_tokenize, wsj_nltk_tokenize_wordpunct_tokenize, spacy_en_tokens, regex_en_tokens]\n",
    "tokenizer_names = ['Ground Truth', 'Nltk word_tokenize', 'Nltk tokenize.wordpunct_tokenize', 'Spacy Tokenization', 'Custom Regex Tokenization']\n",
    "accuracy_perc = [get_tokenization_accuracy(wjs_correct_tokenization, i)[0] for i in wsj_tokenizations]\n",
    "tokenization_lengths = get_tokenization_lens(wsj_tokenizations)\n",
    "# This takes a little while\n",
    "wsj_unique_token_perc = batch_unique_token_percentages(wsj_tokenizations)\n",
    "type_lengths =  get_type_lens(wsj_tokenizations)\n",
    "\n",
    "wsj_results = {'Tokenization Accuracy' : [], \"Token List Length\" : [], \"Type List Length\":[], \"Unique Token Percentage\":[]}\n",
    "for i in range(len(wsj_tokenizations)):\n",
    "    wsj_results['Tokenization Accuracy'].append(str(accuracy_perc[i]) + '%')\n",
    "    wsj_results[\"Token List Length\"].append(tokenization_lengths[i])\n",
    "    wsj_results[\"Type List Length\"].append(type_lengths[i])\n",
    "    wsj_results[\"Unique Token Percentage\"].append(wsj_unique_token_perc[i])\n",
    "print('Wall Street Journal Tokenization Results:\\n')\n",
    "pd.DataFrame(wsj_results, index = tokenizer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3901048e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth 30 most common tokens in order are:\n",
      "\n",
      "[',', 'the', '.', 'of', 'to', 'a', 'in', 'and', '*-1', '0', '*', \"'s\", 'for', 'that', '*T*-1', '*U*', '$', 'The', '``', \"''\", 'is', 'said', 'on', 'it', '%', 'by', 'at', 'with', 'from', 'as']\n",
      "\n",
      "\n",
      "Nltk word_tokenize 30 most common tokens in order are:\n",
      "\n",
      "[',', 'the', '.', 'of', 'to', 'a', 'in', 'and', \"''\", \"'s\", 'for', 'that', 'The', '$', 'is', 'said', 'on', 'it', '%', 'by', '``', 'at', 'with', 'from', 'as', 'million', 'Mr.', 'are', 'was', 'be']\n",
      "\n",
      "\n",
      "Nltk tokenize.wordpunct_tokenize 30 most common tokens in order are:\n",
      "\n",
      "['.', ',', 'the', 'of', 'to', 'a', 'in', 'and', \"'\", '-', \"''\", 's', 'for', 'that', 'The', '$', 'is', 'said', 'on', 'it', 'by', 'at', 'with', 'from', 'million', 'as', 'Mr', '%', 'be', 'was']\n",
      "\n",
      "\n",
      "Spacy Tokenization 30 most common tokens in order are:\n",
      "\n",
      "[',', 'the', '.', 'of', 'to', 'a', 'in', 'and', \"''\", '-', \"'s\", 'for', 'that', 'The', '$', 'is', 'said', 'on', 'it', '%', 'by', 'at', 'with', 'from', 'million', 'as', 'Mr.', 'are', 'was', 'be']\n",
      "\n",
      "\n",
      "Custom Regex Tokenization 30 most common tokens in order are:\n",
      "\n",
      "['.', ',', \"'\", 'the', 'of', 'to', 'a', '-', 'in', 'and', 's', 'for', 'that', 'The', 'is', 'said', 'on', 'it', 'by', 'at', 'with', 'from', 'million', 'as', 'Mr', '%', 'be', 'was', 'are', 'its']\n",
      "\n",
      "\n",
      "The 30 most common tokens amongst all tokenizetions are:\n",
      "\n",
      "[',', '.', 'the', 'of', 'to', 'a', 'in', 'and', \"'\", '-', 'for', 'that', \"''\", 'The', 'is', 'said', '$', \"'s\", 'on', 'it', 'by', '%', 'at', 'with', 'from', 'million', 'as', 'was', 'are', 'be']\n"
     ]
    }
   ],
   "source": [
    "# Getting the 30 most common tokens per tokenization\n",
    "for i in range(5):\n",
    "    print(tokenizer_names[i] + ' 30 most common tokens in order are:\\n')\n",
    "    print(get_ind_common_tokens(wsj_tokenizations[i]))\n",
    "    print('\\n')\n",
    "    \n",
    "# Getting 30 most common tokens overall\n",
    "print('The 30 most common tokens amongst all tokenizetions are:\\n')\n",
    "print(get_total_common_tokens([i for i in wsj_tokenizations]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "49b889e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 tokens that word_tokenize() fails to identify: \n",
      "\n",
      "['*T*-79', '*-126', '120,000', '*-166', 'R.P.', '*T*-98', '@', 'Chafic', '*T*-91', '*T*-255']\n",
      "\n",
      "A common theme is that both nltk and spacy tokenizers struggle when handling spacial charachters such as \"*\" and \"-\".\n"
     ]
    }
   ],
   "source": [
    "# To examine which tokens don't appear in a tokenization\n",
    "wsj_nltk_word_tokenize_missing, wsj_nltk_tokenize_wordpunct_tokenize_missing, spacy_en_tokens_missing, regex_en_tokens_missing = [get_tokenization_accuracy(wjs_correct_tokenization, i)[1] for i in wsj_tokenizations[1:]]\n",
    "\n",
    "# For example, here are 10 tokens that word_tokenize() fails to identify\n",
    "print('10 tokens that word_tokenize() fails to identify: \\n')\n",
    "print(wsj_nltk_word_tokenize_missing[:10])\n",
    "print('\\nA common theme is that both nltk and spacy tokenizers struggle when handling spacial charachters such as \"*\" and \"-\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267ab4c3",
   "metadata": {},
   "source": [
    "<h4>Vima Tokenization<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cb724b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vima Tokenization Results:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenization Accuracy</th>\n",
       "      <th>Token List Length</th>\n",
       "      <th>Type List Length</th>\n",
       "      <th>Unique Token Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ground Truth</th>\n",
       "      <td>100.0%</td>\n",
       "      <td>100676</td>\n",
       "      <td>3861</td>\n",
       "      <td>71.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nltk word_tokenize</th>\n",
       "      <td>97.68%</td>\n",
       "      <td>93530</td>\n",
       "      <td>3886</td>\n",
       "      <td>71.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nltk tokenize.wordpunct_tokenize</th>\n",
       "      <td>97.05%</td>\n",
       "      <td>100637</td>\n",
       "      <td>3897</td>\n",
       "      <td>71.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spacy Tokenization</th>\n",
       "      <td>98.21%</td>\n",
       "      <td>95892</td>\n",
       "      <td>3877</td>\n",
       "      <td>71.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Custom Regex Tokenization</th>\n",
       "      <td>90.69%</td>\n",
       "      <td>98753</td>\n",
       "      <td>3923</td>\n",
       "      <td>72.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Tokenization Accuracy  Token List Length  \\\n",
       "Ground Truth                                    100.0%             100676   \n",
       "Nltk word_tokenize                              97.68%              93530   \n",
       "Nltk tokenize.wordpunct_tokenize                97.05%             100637   \n",
       "Spacy Tokenization                              98.21%              95892   \n",
       "Custom Regex Tokenization                       90.69%              98753   \n",
       "\n",
       "                                  Type List Length  Unique Token Percentage  \n",
       "Ground Truth                                  3861                    71.95  \n",
       "Nltk word_tokenize                            3886                    71.95  \n",
       "Nltk tokenize.wordpunct_tokenize              3897                    71.39  \n",
       "Spacy Tokenization                            3877                    71.83  \n",
       "Custom Regex Tokenization                     3923                    72.32  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vima_tokenizations = [vima_correct_tokenization, vima_nltk_word_tokenize, vima_nltk_tokenize_wordpunct_tokenize, spacy_gr_tokens, regex_gr_tokens]\n",
    "tokenizer_names = ['Ground Truth', 'Nltk word_tokenize', 'Nltk tokenize.wordpunct_tokenize', 'Spacy Tokenization', 'Custom Regex Tokenization']\n",
    "accuracy_perc = [get_tokenization_accuracy(vima_correct_tokenization, i)[0] for i in vima_tokenizations]\n",
    "tokenization_lengths = get_tokenization_lens(wsj_tokenizations)\n",
    "# This takes a little while\n",
    "unique_token_perc = batch_unique_token_percentages(vima_tokenizations)\n",
    "type_lengths =  get_type_lens(vima_tokenizations)\n",
    "\n",
    "vima_results = {'Tokenization Accuracy' : [], \"Token List Length\" : [], \"Type List Length\":[], \"Unique Token Percentage\":[]}\n",
    "for i in range(len(vima_tokenizations)):\n",
    "    vima_results['Tokenization Accuracy'].append(str(accuracy_perc[i]) + '%')\n",
    "    vima_results[\"Token List Length\"].append(tokenization_lengths[i])\n",
    "    vima_results[\"Type List Length\"].append(type_lengths[i])\n",
    "    vima_results[\"Unique Token Percentage\"].append(unique_token_perc[i])\n",
    "print('Vima Tokenization Results:\\n')\n",
    "pd.DataFrame(vima_results, index = tokenizer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "83a7424e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth 30 most common tokens in order are:\n",
      "\n",
      "[',', 'και', '.', 'να', 'την', 'της', 'σ', 'το', 'του', 'των', 'που', 'με', 'η', 'τα', 'για', 'τους', 'τη', 'τον', 'τις', 'από', 'θα', 'δεν', 'είναι', 'σε', 'οι', 'ο', '«', 'μας', '»', 'ότι']\n",
      "\n",
      "\n",
      "Nltk word_tokenize 30 most common tokens in order are:\n",
      "\n",
      "[',', 'και', '.', 'να', 'της', 'του', 'την', 'το', 'των', 'που', 'με', 'η', 'για', 'τα', 'τους', 'από', 'θα', 'δεν', 'είναι', 'σε', 'τον', 'τη', 'τις', 'οι', 'ο', '«', '»', 'μας', 'ότι', 'στην']\n",
      "\n",
      "\n",
      "Nltk tokenize.wordpunct_tokenize 30 most common tokens in order are:\n",
      "\n",
      "[',', 'και', '.', 'να', 'της', 'του', 'την', 'το', 'των', 'που', 'με', 'η', 'για', 'τα', 'τους', 'από', 'θα', 'δεν', 'είναι', 'σε', 'τον', 'τη', 'τις', 'οι', 'ο', 'μας', '«', 'ότι', 'στην', 'στο']\n",
      "\n",
      "\n",
      "Spacy Tokenization 30 most common tokens in order are:\n",
      "\n",
      "[',', 'και', '.', 'να', 'της', 'του', 'την', 'το', 'των', 'που', 'με', 'η', 'για', 'τα', 'τους', 'από', 'θα', 'δεν', 'είναι', 'σε', 'τον', 'τη', 'τις', 'οι', 'ο', '«', '»', 'μας', 'ότι', 'στην']\n",
      "\n",
      "\n",
      "Custom Regex Tokenization 30 most common tokens in order are:\n",
      "\n",
      "[',', 'και', '.', 'να', 'της', 'του', 'την', 'το', 'των', 'που', 'με', 'η', 'για', 'ύ', 'τα', 'τους', 'από', 'θα', 'δεν', 'είναι', 'σε', 'τον', 'τη', 'τις', 'οι', 'ο', 'μας', 'ότι', 'στην', 'στο']\n",
      "\n",
      "\n",
      "The 30 most common tokens amongst all tokenizetions are:\n",
      "\n",
      "[',', 'και', '.', 'να', 'της', 'του', 'την', 'το', 'των', 'που', 'με', 'η', 'για', 'τα', 'τους', 'από', 'θα', 'δεν', 'είναι', 'τον', 'σε', 'τη', 'τις', 'οι', 'ο', 'μας', 'ότι', '»', '«', 'Η']\n"
     ]
    }
   ],
   "source": [
    "# Getting the 30 most common tokens per tokenization\n",
    "for i in range(5):\n",
    "    print(tokenizer_names[i] + ' 30 most common tokens in order are:\\n')\n",
    "    print(get_ind_common_tokens(vima_tokenizations[i]))\n",
    "    print('\\n')\n",
    "    \n",
    "# Getting 30 most common tokens overall\n",
    "print('The 30 most common tokens amongst all tokenizetions are:\\n')\n",
    "print(get_total_common_tokens([i for i in vima_tokenizations]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "765166d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 tokens that the regex tokenizer fails to identify: \n",
      "\n",
      "['λ.χ', 'ηθικού', '113%', \"καθ'\", 'αιματηρές(', 'στρατού', 'ματ', 'στενού', 'εργατικού', '15,1%']\n",
      "\n",
      "Again, the common theme is a difficulty when dealing with special characters and symbols\n"
     ]
    }
   ],
   "source": [
    "# Tokens that don't appear in a tokenization\n",
    "vima_nltk_word_tokenize_missing, vima_nltk_tokenize_wordpunct_tokenize_missing, spacy_gr_tokens_missing, regex_gr_tokens_missing = [get_tokenization_accuracy(vima_correct_tokenization, i)[1] for i in vima_tokenizations[1:]]\n",
    "\n",
    "# For example, here are 10 tokens that word_tokenize() fails to identify\n",
    "print('10 tokens that the regex tokenizer fails to identify: \\n')\n",
    "print(regex_gr_tokens_missing[:10])\n",
    "print('\\nAgain, the common theme is a difficulty when dealing with special characters and symbols')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d660db",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1dcab6",
   "metadata": {},
   "source": [
    "<h2>Sentence Boundary Disambiguation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f17b74b",
   "metadata": {},
   "source": [
    "<h3>Function to extract Sentence Boundary Disambiguation metrics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "63572fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aewrawerwear', 'erwar']\n",
      "['a', 'tree', 'is']\n",
      "['aewrawerwear erwar', 'a tree is']\n"
     ]
    }
   ],
   "source": [
    "# Helper for wsj ground trouth\n",
    "def remove_if_contains(the_list, character):\n",
    "    for i in range(len(the_list)):\n",
    "        list_words = the_list[i].split()\n",
    "\n",
    "        for j in reversed(range(len(list_words))):\n",
    "\n",
    "            if character in str(list_words[j]):\n",
    "                list_words.pop(j)\n",
    "        print(list_words)    \n",
    "        the_list[i] = ' '.join(list_words)\n",
    "    return the_list\n",
    "# Get Metrics of a list\n",
    "def get_presition(gt, other_sbd):\n",
    "    unified_set = set(list(itertools.chain(gt, other_sbd)))\n",
    "    ot_set = set(other_sbd)\n",
    "    cor_set = set(gt)\n",
    "    tp = ot_set.intersection(cor_set)\n",
    "    fp = ot_set.difference(cor_set)\n",
    "    fn = cor_set.difference(ot_set)\n",
    "    precision = round((len(tp)/ (len(tp)+len(fp))),3)\n",
    "    print(list(fn)[:3])\n",
    "    return precision\n",
    "\n",
    "def get_recall(gt, other_sbd):\n",
    "    unified_set = set(list(itertools.chain(gt, other_sbd)))\n",
    "    ot_set = set(other_sbd)\n",
    "    cor_set = set(gt)\n",
    "    tp = ot_set.intersection(cor_set)\n",
    "    fp = ot_set.difference(cor_set)\n",
    "    fn = cor_set.difference(ot_set)\n",
    "    recall = round((len(tp)/ (len(tp)+len(fn))),3)\n",
    "    return recall\n",
    "\n",
    "def get_f1(gt, other_sbd):\n",
    "    f1_score = 2*(get_presition(gt, other_sbd)*get_recall(gt, other_sbd)/(get_presition(gt, other_sbd)+get_recall(gt, other_sbd)))\n",
    "    return f1_score\n",
    "\n",
    "\n",
    "# Get average length of strings of a list\n",
    "def get_avg_len(sbd_list):\n",
    "    return round(sum(map(len, sbd_list))/float(len(sbd_list)),2)\n",
    "\n",
    "# Get min length of strings in lists\n",
    "def get_min_len(sbd_list):\n",
    "    return len(min(sbd_list, key=len))\n",
    "\n",
    "# Get max length of strings in list\n",
    "def get_max_len(sbd_list):\n",
    "    return len(max(sbd_list, key=len))\n",
    "\n",
    "# Get average number of tokens per sentence\n",
    "def avg_tokens(sbd_list):\n",
    "    token_len = [len(i) for i in [word_tokenize(i) for i in sbd_list]]\n",
    "    return round(sum(token_len)/len(token_len),2)\n",
    "\n",
    "# Get max number of tokens of sbd\n",
    "def max_tokens(sbd_list):\n",
    "    return max([len(i) for i in [word_tokenize(i) for i in sbd_list]])\n",
    "\n",
    "# Get min number of tokens of sbd\n",
    "def min_tokens(sbd_list):\n",
    "    return min([len(i) for i in [word_tokenize(i) for i in sbd_list]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c496287e",
   "metadata": {},
   "source": [
    "<h3>Getting Correct Sentence Boundaries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "ecbb7bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.', 'Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group.', 'Rudolph Agnew, 55 years old and former chairman of Consolidated Gold Fields PLC, was named *-1 a nonexecutive director of this British industrial conglomerate.', 'A form of asbestos once used * * to make Kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed * to it more than 30 years ago, researchers reported 0 *T*-1.', 'The asbestos fiber, crocidolite, is unusually resilient once it enters the lungs, with even brief exposures to it causing symptoms that *T*-1 show up decades later, researchers said 0 *T*-2.']\n"
     ]
    }
   ],
   "source": [
    "# Get correct Sentence Boundaries\n",
    "vima_correct_sbd = re.sub('\\s*([.;,])', r'\\1', ''.join([(open('assignment1textfiles/sbd/' + i, encoding=\"utf8\").read().strip()) for i in os.listdir('assignment1textfiles/sbd')]).replace('\\n',' ')).split('<S>')\n",
    "for i in range(len(vima_correct_sbd)):\n",
    "    vima_correct_sbd[i] = vima_correct_sbd[i].strip()\n",
    "# wsj_correct_sbd = [re.sub('\\s*([.;,])',r'\\1', ' '.join(i)) for i in [item for sublist in [treebank.sents(treebank.fileids())] for item in sublist]]\n",
    "\n",
    "wsj_correct_sbd = [re.sub('\\s*([.;,])',r'\\1', ' '.join(i)) for i in treebank.sents()]\n",
    "\n",
    "print(wsj_correct_sbd[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd7b6a0",
   "metadata": {},
   "source": [
    "<h3>Sentence Boundary Disambiguation when dealing with English</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "656225fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "962792d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk sent_tokenize\n",
    "wsj_nltk_sent_tokenize = sent_tokenize(wsj_raw_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "266b48fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk PunktSentenceTokenizer\n",
    "wsj_nltk_punkt_sent_tokenize = PunktSentenceTokenizer(wsj_raw_text.strip()).sentences_from_text(wsj_raw_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "007b6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy sentence boundary disambiguation\n",
    "spacy_en_sbd = [str(i.sent).strip() for i in nlp(wsj_raw_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "f7083b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom regex sentence boundary disambiguation\n",
    "sbd_en_regex = re.findall(r\"[A-Z].*?[\\.!?] \", wsj_raw_text) # A space is included at the end so that numbers with decimals won't trigger a new sentence\n",
    "for i in range(len(sbd_en_regex)):\n",
    "    sbd_en_regex[i] = sbd_en_regex[i][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791e76c",
   "metadata": {},
   "source": [
    "<h3>Sentence Boundary Disambiguation when dealing with Greek</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8bbc7a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk sent_tokenize\n",
    "vima_nltk_sent_tokenize = sent_tokenize(vima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "54403cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk PunktSentenceTokenizer\n",
    "vima_nltk_punkt_sent_tokenize = PunktSentenceTokenizer(vima).sentences_from_text(vima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "08702a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy sentence boundary disambiguation\n",
    "spacy_gr_sbd = [str(i.sent) for i in nlp(vima).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "c5bdc298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom regex sentence boundary disambiguation\n",
    "sbd_gr_regex = re.findall(r\"[Α-Ω].*?[\\.!?]\", vima)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed048c78",
   "metadata": {},
   "source": [
    "<h3>Sentence Boundary Disambiguation Comparison</h3>\n",
    "<h4>Wall Street Journal SBD<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "472a91b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall Street Journal SBD Results:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Min Length</th>\n",
       "      <th>Max Length</th>\n",
       "      <th>Average Length</th>\n",
       "      <th>Min Tokens</th>\n",
       "      <th>Max Tokens</th>\n",
       "      <th>Average Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ground Truth</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1294</td>\n",
       "      <td>135.78</td>\n",
       "      <td>1</td>\n",
       "      <td>311</td>\n",
       "      <td>27.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nltk sent_tokenize</th>\n",
       "      <td>0.140</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.138993</td>\n",
       "      <td>2</td>\n",
       "      <td>1157</td>\n",
       "      <td>128.69</td>\n",
       "      <td>2</td>\n",
       "      <td>234</td>\n",
       "      <td>24.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nltk PunktSentenceTokenizer</th>\n",
       "      <td>0.128</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.129483</td>\n",
       "      <td>2</td>\n",
       "      <td>1157</td>\n",
       "      <td>123.01</td>\n",
       "      <td>2</td>\n",
       "      <td>234</td>\n",
       "      <td>23.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spacy SBD</th>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>1</td>\n",
       "      <td>1160</td>\n",
       "      <td>158.21</td>\n",
       "      <td>1</td>\n",
       "      <td>234</td>\n",
       "      <td>29.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Custom Regex SBD</th>\n",
       "      <td>0.099</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>2</td>\n",
       "      <td>775</td>\n",
       "      <td>102.81</td>\n",
       "      <td>2</td>\n",
       "      <td>141</td>\n",
       "      <td>19.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Precision  Recall  F1 Score  Min Length  \\\n",
       "Ground Truth                     1.000   1.000  1.000000           1   \n",
       "Nltk sent_tokenize               0.140   0.138  0.138993           2   \n",
       "Nltk PunktSentenceTokenizer      0.128   0.131  0.129483           2   \n",
       "Spacy SBD                        0.137   0.137  0.137000           1   \n",
       "Custom Regex SBD                 0.099   0.112  0.105100           2   \n",
       "\n",
       "                             Max Length  Average Length  Min Tokens  \\\n",
       "Ground Truth                       1294          135.78           1   \n",
       "Nltk sent_tokenize                 1157          128.69           2   \n",
       "Nltk PunktSentenceTokenizer        1157          123.01           2   \n",
       "Spacy SBD                          1160          158.21           1   \n",
       "Custom Regex SBD                    775          102.81           2   \n",
       "\n",
       "                             Max Tokens  Average Tokens  \n",
       "Ground Truth                        311           27.99  \n",
       "Nltk sent_tokenize                  234           24.21  \n",
       "Nltk PunktSentenceTokenizer         234           23.20  \n",
       "Spacy SBD                           234           29.62  \n",
       "Custom Regex SBD                    141           19.57  "
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsj_sbds = [wsj_correct_sbd, wsj_nltk_sent_tokenize, wsj_nltk_punkt_sent_tokenize, spacy_en_sbd, sbd_en_regex]\n",
    "tokenizer_names = ['Ground Truth', 'Nltk sent_tokenize', 'Nltk PunktSentenceTokenizer', 'Spacy SBD', 'Custom Regex SBD']\n",
    "presc = [get_presition(wsj_correct_sbd, i) for i in wsj_sbds]\n",
    "recall = [get_recall(wsj_correct_sbd, i) for i in wsj_sbds]\n",
    "f1 = [get_f1(wsj_correct_sbd, i) for i in wsj_sbds]\n",
    "min_len = [get_min_len(i) for i in wsj_sbds]\n",
    "max_len = [get_max_len(i) for i in wsj_sbds]\n",
    "avg_len = [get_avg_len(i) for i in wsj_sbds]\n",
    "min_tok = [min_tokens(i) for i in wsj_sbds]\n",
    "max_tok = [max_tokens(i) for i in wsj_sbds]\n",
    "avg_tok = [avg_tokens(i) for i in wsj_sbds]\n",
    "\n",
    "wsj_results = {'Precision' : [], \"Recall\":[], \"F1 Score\":[], 'Min Length':[], 'Max Length' : [], 'Average Length' : [], 'Min Tokens':[], 'Max Tokens':[],'Average Tokens':[],}\n",
    "for i in range(len(wsj_sbds)):\n",
    "    wsj_results['Precision'].append(presc[i])\n",
    "    wsj_results[\"Recall\"].append(recall[i])\n",
    "    wsj_results[\"F1 Score\"].append(f1[i])\n",
    "    wsj_results[\"Min Length\"].append(min_len[i])\n",
    "    wsj_results[\"Max Length\"].append(max_len[i])\n",
    "    wsj_results[\"Average Length\"].append(avg_len[i])\n",
    "    wsj_results[\"Min Tokens\"].append(min_tok[i])\n",
    "    wsj_results[\"Max Tokens\"].append(max_tok[i])\n",
    "    wsj_results[\"Average Tokens\"].append(avg_tok[i])\n",
    "\n",
    "print('Wall Street Journal SBD Results:\\n')\n",
    "pd.DataFrame(wsj_results, index = tokenizer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "85317cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vima SBD Results:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Min Length</th>\n",
       "      <th>Max Length</th>\n",
       "      <th>Average Length</th>\n",
       "      <th>Min Tokens</th>\n",
       "      <th>Max Tokens</th>\n",
       "      <th>Average Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ground Truth</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>752</td>\n",
       "      <td>191.77</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>32.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nltk sent_tokenize</th>\n",
       "      <td>0.351</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.352989</td>\n",
       "      <td>4</td>\n",
       "      <td>742</td>\n",
       "      <td>187.23</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>31.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nltk PunktSentenceTokenizer</th>\n",
       "      <td>0.361</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.355415</td>\n",
       "      <td>6</td>\n",
       "      <td>742</td>\n",
       "      <td>196.32</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>32.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spacy SBD</th>\n",
       "      <td>0.280</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.298667</td>\n",
       "      <td>2</td>\n",
       "      <td>742</td>\n",
       "      <td>165.58</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>27.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Custom Regex SBD</th>\n",
       "      <td>0.311</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.323017</td>\n",
       "      <td>2</td>\n",
       "      <td>742</td>\n",
       "      <td>167.22</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>28.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Precision  Recall  F1 Score  Min Length  \\\n",
       "Ground Truth                     1.000   1.000  1.000000           0   \n",
       "Nltk sent_tokenize               0.351   0.355  0.352989           4   \n",
       "Nltk PunktSentenceTokenizer      0.361   0.350  0.355415           6   \n",
       "Spacy SBD                        0.280   0.320  0.298667           2   \n",
       "Custom Regex SBD                 0.311   0.336  0.323017           2   \n",
       "\n",
       "                             Max Length  Average Length  Min Tokens  \\\n",
       "Ground Truth                        752          191.77           0   \n",
       "Nltk sent_tokenize                  742          187.23           2   \n",
       "Nltk PunktSentenceTokenizer         742          196.32           2   \n",
       "Spacy SBD                           742          165.58           2   \n",
       "Custom Regex SBD                    742          167.22           2   \n",
       "\n",
       "                             Max Tokens  Average Tokens  \n",
       "Ground Truth                        133           32.59  \n",
       "Nltk sent_tokenize                  130           31.40  \n",
       "Nltk PunktSentenceTokenizer         130           32.92  \n",
       "Spacy SBD                           130           27.84  \n",
       "Custom Regex SBD                    130           28.18  "
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vima_sbds = [vima_correct_sbd, vima_nltk_sent_tokenize, vima_nltk_punkt_sent_tokenize, spacy_gr_sbd, sbd_gr_regex]\n",
    "tokenizer_names = ['Ground Truth', 'Nltk sent_tokenize', 'Nltk PunktSentenceTokenizer', 'Spacy SBD', 'Custom Regex SBD']\n",
    "presc = [get_presition(vima_correct_sbd, i) for i in vima_sbds]\n",
    "recall = [get_recall(vima_correct_sbd, i) for i in vima_sbds]\n",
    "f1 = [get_f1(vima_correct_sbd, i) for i in vima_sbds]\n",
    "min_len = [get_min_len(i) for i in vima_sbds]\n",
    "max_len = [get_max_len(i) for i in vima_sbds]\n",
    "avg_len = [get_avg_len(i) for i in vima_sbds]\n",
    "min_tok = [min_tokens(i) for i in vima_sbds]\n",
    "max_tok = [max_tokens(i) for i in vima_sbds]\n",
    "avg_tok = [avg_tokens(i) for i in vima_sbds]\n",
    "\n",
    "vima_results = {'Precision' : [], \"Recall\":[], \"F1 Score\":[], 'Min Length':[], 'Max Length' : [], 'Average Length' : [], 'Min Tokens':[], 'Max Tokens':[],'Average Tokens':[],}\n",
    "for i in range(len(wsj_sbds)):\n",
    "    vima_results['Precision'].append(presc[i])\n",
    "    vima_results[\"Recall\"].append(recall[i])\n",
    "    vima_results[\"F1 Score\"].append(f1[i])\n",
    "    vima_results[\"Min Length\"].append(min_len[i])\n",
    "    vima_results[\"Max Length\"].append(max_len[i])\n",
    "    vima_results[\"Average Length\"].append(avg_len[i])\n",
    "    vima_results[\"Min Tokens\"].append(min_tok[i])\n",
    "    vima_results[\"Max Tokens\"].append(max_tok[i])\n",
    "    vima_results[\"Average Tokens\"].append(avg_tok[i])\n",
    "\n",
    "print('Vima SBD Results:\\n')\n",
    "pd.DataFrame(vima_results, index = tokenizer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078880f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
